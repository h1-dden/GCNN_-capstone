{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing Twitter data and create a CSV file with tweet text and sentiment labels for use in text embedding through LSTM:\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the Twitter data\n",
    "data = pd.read_csv('covid_data.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Extract unique users\n",
    "unique_users = data['text'].unique()\n",
    "\n",
    "# Keep only the columns with tweet text and author\n",
    "data = data[['screen_name','text','location','friends_count']]\n",
    "\n",
    "# Define a function to clean the tweet text\n",
    "def clean_tweet(text):\n",
    "    # Remove mentions, hashtags, and URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', text)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Clean the tweet text\n",
    "data['text'] = data['text'].apply(clean_tweet)\n",
    "\n",
    "# Define a function to get the sentiment label\n",
    "def get_sentiment(text):\n",
    "    # Use TextBlob to get the sentiment polarity\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    # Classify as positive (1), negative (0), or neutral (2)\n",
    "    if sentiment > 0:\n",
    "        return 1\n",
    "    elif sentiment < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Get the sentiment labels\n",
    "data['sentimental_value'] = data['text'].apply(get_sentiment)\n",
    "\n",
    "#data['sentimental_value'] = data.groupby('author')['label'].agg(lambda x: x.mode()[0])\n",
    "\n",
    "\n",
    "# Keep only the columns with clean tweet text and sentiment labels\n",
    "data = data[['screen_name','text', 'sentimental_value','location','friends_count']]\n",
    "\n",
    "# Save the preprocessed data to a CSV file\n",
    "data.to_csv('preprocessed_twitter_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19131df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of text embedding through LSTM for Twitter data using Keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "plt.style.use('fivethirtyeight')\n",
    "from keras.models import Model\n",
    "\n",
    "# Load the Twitter data\n",
    "data = pd.read_csv('preprocessed_twitter_data.csv')\n",
    "\n",
    "print(\"processing data...\")\n",
    "sns.countplot(x='sentimental_value',data=data)\n",
    "\n",
    "\n",
    "# Define the maximum number of words to keep in the vocabulary\n",
    "vocab_size = len(data)\n",
    "\n",
    "# Define the maximum length of a tweet\n",
    "max_length = 1000\n",
    "\n",
    "# Define the embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['text'], data['sentimental_value'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Tokenize the training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Tokenize the testing data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), epochs=epochs, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_output(history, epochs):\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epochs + 1), history['loss'], label='Training Loss')\n",
    "    plt.plot(range(1, epochs + 1), history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epochs + 1), history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(range(1, epochs + 1), history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_padded, test_labels, verbose=0)\n",
    "print('Test accuracy: %f' % (accuracy*100))\n",
    "\n",
    "#Plot the model\n",
    "metric_names = ['loss', 'val_loss', 'accuracy', 'val_accuracy']\n",
    "plot_labels = ['Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "plot_model_output(history.history, epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ed6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embeddings = model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_embeddings)\n",
    "print(len(lstm_embeddings))\n",
    "print(len(lstm_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f92892",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"lstm_embedding.txt\",lstm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae861d",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "\n",
    "1.Loads the Twitter data from a CSV file.\n",
    "2.Defines the maximum number of words to keep in the vocabulary, the maximum length of a tweet, and the embedding dimension.\n",
    "3.Splits the data into training and testing sets.\n",
    "4.Tokenizes the training and testing data using Keras' Tokenizer class.\n",
    "5.Pads the sequences to a fixed length using Keras' pad_sequences function.\n",
    "6.Defines an LSTM model using Keras' Sequential API.\n",
    "7.Compiles the model with binary cross-entropy loss and the Adam optimizer.\n",
    "8.Trains the model on the training data.\n",
    "9.Evaluates the model on the testing data and prints the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c1321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
